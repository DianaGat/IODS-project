---
output: html_document
---
# Clustering  
### Description of the dataset  
```{r}
date()
```

Load data, explore the dataset, and draw the plot matrix of the variables.  
```{r}
# access the MASS package
install.packages("corrplot", 
                 repos = "https://cran.rstudio.com/bin/windows/contrib/4.1/corrplot_0.92.zip")

library(MASS)
library(GGally)
library(ggplot2)
library(corrplot)
library(tidyverse)
library(cowplot)

# load the data
data("Boston")
data <- Boston
# explore the dataset
str(data) 
```  
Data has one integer (rad), one dichotomous (chas) and the rest are numeric variables.  
Name of the dataset is "Housing Values in Suburbs of Boston". Dataset has 506 observations and 14 variables. It describes housing Values in Suburbs of Boston.    
This data frame contains the following columns:  

crim - per capita crime rate by town.  
zn - proportion of residential land zoned for lots over 25,000 sq.ft.  
indus - proportion of non-retail business acres per town.  
chas - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).  
nox - nitrogen oxides concentration (parts per 10 million).  
rm - average number of rooms per dwelling.  
age - proportion of owner-occupied units built prior to 1940.  
dis - weighted mean of distances to five Boston employment centres.  
rad - index of accessibility to radial highways.  
tax - full-value property-tax rate per \$10,000.  
ptratio - pupil-teacher ratio by town.  
black - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.  
lstat - lower status of the population (percent).  
medv - median value of owner-occupied homes in \$1000s.  

Sources of the dataset:  
1) Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81–102.  
2) Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.  

More information about the variables in the Boston dataset can be found by the following link: <https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html>.  
### Overview of the data  
```{r}
summary(data)

# plot matrix of the variables

### ggpairs(data, mapping = aes(), lower = list(combo = wrap("facethist", bins = 20)))
# it was suggested on DataCamp but I don`t seeing necessity in this, since the plots are small and hardly interpretative, moreover, I will have correlation plot separately for the dataset. 

# Other than matrix plot, I will look at the distribution separately.
gather(data) %>% ggplot(aes(x = value))  + geom_histogram(aes(y = ..density..), colour="black", fill="white", bins = 18,) +  geom_density(alpha = 0.3, fill = "black") + facet_wrap("key", scales = "free")

boxplot(data)
```  
We can see that “crime,” “zn,” “chaz,” “dis,” and “black” are highly skewed. Black - skewed to right - there is high proportion of black people in town. Crim - slewed to left - low crime rate in town. Zn - skewed to lest - low proportion of of residential leand zoned for lots over 25.000 sq.ft. Chas - skewed to 0 - there are more suburbs that are not bound to Charles River. Dis - skewed to left - average mean of distances to five Boston employment centres is higher that the median.  
Several other variables have moderate skewness. Age is slightly skewed to right indicating higher proportion of owner occupeted built prior to 1940. Medv (median value of owner-occupied homes in 1000s) and rm (average number of rooms per dwelling)  have normal distribution. Variables tax (full-value property-tax rate per 10,000) and rad (index of accessibility to radial highways) are normally distributed but have some outlines. Nox - skewed to left - there is lower proportion of nitrogen oxides concentration in the Boston suburbs. Ptratio is slightly skewed to left - there is a higher proportion of pupil-teacher ratio by town. Distribution of indus variable has two pikes that indicates that proportion of non-retail business acres per town the proportion is either approx. 5 or approx. 20. Istat is slightly skewed to left - the present of lower status of the population is smaller than median.  

Below we can see correlation table for the variables.  
```{r}
# calculate the correlation matrix and round it
cor_matrix<-cor(data) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper" , cl.pos="b" , tl.pos="d", tl.cex = 0.6)
```  
There is strong correlation between nox and indus (r = 0.760) indicating that the higher the nitrogen oxides concentration, the higher the proportion of non-retail business acres per town;  
Nox and age (r= 0.730) indicationg that the older the age of the buildings, the higher the nitrogen oxides concentration;  
Nox and dis (r= -0.770) indicationg that the longer the distance from the Boston employment centres, the lower the nitrogen oxides concentration and the proportion of non-retail business acres per town;  

Age and dis (r = -0.750) indicating that the longer the distance from the Boston employment centres, the lower the age of the buildings;  

Tax and indus (r = 0.720) meaning that the higher the full-value property-tax rate, the bigger the proportion of non-retail business acres per town;  
Tax and rad (r= 0.910) indicationg that the higher the full-value property-tax rate, the better the accessibility to radial highways;  

Medv and rm (r = 0.700) indicating that the higher the median value of owner-occupied homes, the higher the average number of rooms per dwelling;  
Medv and lstat (r= -0.740) meaning that the higher the median value of owner-occupied homes, the lower the proportion of the lower status population.  

### Standartization of the dataset  
```{r}
# center and standardize variables
data_scaled <- scale(data)

# summaries of the scaled variables
summary(data_scaled)

# class of the boston_scaled object
class(data_scaled)

# change the object to data frame
data_scaled <- as.data.frame(data_scaled)
data_scaled_k <- as.data.frame(data_scaled)
```  
After scaling we made that each variable has mean 0. The next step, it to categorize the crim using quantities as break points.  

```{r}
# create a quantile vector of crim and print it
bins <- quantile(data_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(data_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)
# remove original crim from the dataset
data_scaled <- dplyr::select(data_scaled, -crim)

# add the new categorical value to scaled data
data_scaled <- data.frame(data_scaled, crime)
str(data_scaled)
```  
We added a new column "crime" with categorized continuous "crim". Next, I will devide dataset to train and drop sets (train set <-80% of dataset).  
```{r}
# number of rows in the Boston dataset 
n <- nrow(data_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- data_scaled[ind,]

# create test set 
test <- data_scaled[-ind,]
```  
Now we have two datasets for analysis and tasting.  

## Linear Discriminant analysis  
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 3)

# save and remove crime categories
test_correct = test$crim
test$crim = NULL
```  
### Prediction  
```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = test_correct, predicted = lda.pred$class)
```  
Cross tabulation analysis shows good results, however, we can see that some classesare missclassified, but they are located in neighboring classes.  

### K means  
I saved scaled data before, so I will just use it now without repeating the procedure.  
```{r}
summary(data_scaled_k)

# euclidean distance matrix
dist_eu <- dist(data_scaled_k)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(data_scaled_k, method = 'manhattan')

# look at the summary of the distances
summary(dist_man)
```  
No we can see distances and its summary between the obseravtions. Next, we will run K-means clustering.  
```{r}
km <- kmeans(na.omit(data_scaled_k), 3)

# plot the data_scaled_k dataset with clusters
pairs(data_scaled_k, col = km$cluster)

```  
In every plot, the different colors focus on different regions of the plot. But anyway, I will try to find the optimal number of clusters due to some overlaps.  
```{r}
# data_scaled_k dataset is available
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(data_scaled_k, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

```{r}
# k-means clustering
km <-kmeans(data_scaled_k, centers = 2)

# plot the data_scaled_k dataset with clusters
pairs(data_scaled_k[4:8], col = km$cluster)
```  
We can see that two clusters are good match, there are no outliners that would require one more cluster.